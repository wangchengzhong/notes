wav2vec (Schneider et al., 2019) learns representations of audio data by solving a self-supervised context-prediction task with the same loss function as word2vec (Mikolov et al., 2013; van den Oord et al., 2018). The model is based on two convolutional neural networks where the the encoder produces a representation $\mathbf{z}_i$ for each time step $i$ at a rate of $100 \mathrm{~Hz}$ and the aggregator combines multiple encoder time steps into a new representation $\mathbf{c}_i$ for each time step $i$. Given an aggregated representation $\mathbf{c}_i$, the model is trained to distinguish a sample $\mathbf{z}_{i+k}$ that is $k$ steps in the future from distractor samples $\tilde{\mathbf{z}}$ drawn from a distribution $p_n$, by minimizing the contrastive loss for steps $k=1, \ldots, K$ :
$$
\mathcal{L}_k^{\text {wav2vec }}=-\sum_{i=1}^{T-k}\left(\log \sigma\left(\mathbf{z}_{i+k}^{\top} h_k\left(\mathbf{c}_i\right)\right)+\underset{\tilde{\mathbf{z}} \sim p_n}{\lambda} \mathbb{E}\left[\log \sigma\left(-\tilde{\mathbf{z}}^{\top} h_k\left(\mathbf{c}_i\right)\right)\right]\right)
$$

wav2vec模型的核心是通过自监督学习来学习音频数据的表示，这一过程借鉴了word2vec模型的思想。wav2vec模型的目标是让模型能够通过当前的音频表示来预测未来的音频表示。这里的损失函数是为了实现这一目标而设计的。
损失函数的设计基于对比损失（contrastive loss），这是一种常用于自监督学习的损失函数，用于学习区分正样本和负样本。在wav2vec的场景中，正样本是未来的音频表示$\mathbf{z}{i+k}$，而负样本$\tilde{\mathbf{z}}$是从一个分布$p_n$中随机抽取的，与当前音频表示不直接相关的音频表示。损失函数由两部分组成：1. 第一部分是正样本的对数似然，$\log \sigma\left(\mathbf{z}{i+k}^{\top} h_k\left(\mathbf{c}i\right)\right)$，这里$\sigma$是sigmoid函数，$\mathbf{z}{i+k}^{\top} h_k\left(\mathbf{c}i\right)$计算的是正样本与当前聚合表示$\mathbf{c}_i$通过一个转换函数$h_k$之后的内积。这一项鼓励模型将正样本与当前表示相关联。2. 第二部分是负样本的期望对数似然，$\mathbb{E}\left[\log \sigma\left(-\tilde{\mathbf{z}}^{\top} h_k\left(\mathbf{c}_i\right)\right)\right]$，这里期望是在负样本分布$p_n$上计算的。这一项鼓励模型将负样本与当前表示区分开。$\lambda$是一个正则化参数，用于平衡这两部分的重要性。整个损失函数的目的是最小化正样本与负样本的对比损失，从而让模型学会从当前的音频表示预测未来的音频表示。这种方式使得模型能够捕捉到音频数据中的时间序列特征，而不需要任何外部的标注信息，体现了自监督学习的思想。